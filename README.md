# Neural Language Model - Pride and Prejudice

A PyTorch implementation of neural language models trained on Jane Austen's "Pride and Prejudice" from scratch. This project demonstrates understanding of sequence models, model capacity, and generalization through three experimental scenarios: underfitting, overfitting, and best fit.

## ğŸ“‹ Project Overview

This project implements LSTM-based language models trained to predict text at the character level. The implementation includes:

- **From-scratch PyTorch implementation** (no pre-trained models)
- **Three training scenarios** demonstrating different model behaviors
- **Comprehensive evaluation** with perplexity metrics
- **Text generation** capabilities with various sampling strategies
- **Complete reproducibility** with fixed random seeds

## ğŸ¯ Objectives

1. âœ… Implement neural language models from scratch using PyTorch
2. âœ… Train and evaluate models on provided dataset
3. âœ… Demonstrate underfitting, overfitting, and best fit scenarios
4. âœ… Calculate and compare perplexity metrics
5. âœ… Generate comprehensive training visualizations

## ğŸ“Š Dataset

**Pride and Prejudice by Jane Austen**
- Source: Project Gutenberg
- Preprocessed text length: ~13,000 lines
- Tokenization: Character-level (vocabulary size: ~70 characters)
- Splits: 70% training / 15% validation / 15% test

## ğŸ—ï¸ Architecture

**Model Type:** LSTM-based Language Model

### Three Experimental Scenarios:

#### 1. Underfitting Model
```
Embedding dim: 64
Hidden dim: 128
Layers: 1
Dropout: 0.0
Learning rate: 0.01 (too high)
Epochs: 5 (insufficient)
```
**Expected behavior:** High training AND validation loss

#### 2. Overfitting Model
```
Embedding dim: 512
Hidden dim: 1024
Layers: 4
Dropout: 0.0 (no regularization)
Learning rate: 0.001
Epochs: 50
Batch size: 32 (small)
```
**Expected behavior:** Low training loss, high validation loss (diverging)

#### 3. Best Fit Model
```
Embedding dim: 256
Hidden dim: 512
Layers: 2
Dropout: 0.3
Learning rate: 0.001
Weight decay: 1e-5
Epochs: 30
Early stopping: patience 5
```
**Expected behavior:** Converging training and validation loss

## ğŸš€ Installation

### Prerequisites
- Python 3.8+
- PyTorch 2.0+
- CUDA (optional, for GPU training)

### Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd Assignment_2

# Install dependencies
pip install -r requirements.txt
```

## ğŸ’» Usage

### Training

Train a specific scenario:
```bash
# Train underfit model
python src/train.py --scenario underfit

# Train overfit model
python src/train.py --scenario overfit

# Train best fit model
python src/train.py --scenario best_fit

# Train all scenarios
python src/train.py --scenario all
```

### Evaluation

Evaluate trained models:
```bash
# Evaluate specific scenario
python src/evaluate.py --scenario best_fit

# Evaluate all scenarios
python src/evaluate.py --scenario all
```

### Text Generation

Generate text with trained models:
```bash
# Generate with default settings
python src/generate.py --scenario best_fit

# Generate with custom parameters
python src/generate.py \
    --scenario best_fit \
    --seed "It is a truth universally acknowledged" \
    --length 500 \
    --temperature 0.8 \
    --top_k 50 \
    --num_samples 3
```

### Visualization

Create comparison plots:
```bash
python src/visualize.py
```

## ğŸ“ˆ Results

### Test Perplexity Comparison

| Scenario | Test Perplexity | Test Loss |
|----------|----------------|-----------|
| Underfit | TBD | TBD |
| Overfit | TBD | TBD |
| Best Fit | TBD | TBD |

*Note: Results will be updated after training*

### Training Curves

Training and validation loss plots for all three scenarios are available in the `plots/` directory:
- `underfit_training_curve.png`
- `overfit_training_curve.png`
- `best_fit_training_curve.png`
- `all_scenarios_comparison.png`
- `perplexity_comparison.png`

## ğŸ“ Project Structure

```
Assignment_2/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py          # Hyperparameters and configurations
â”‚   â”œâ”€â”€ data.py            # Data preprocessing and dataset classes
â”‚   â”œâ”€â”€ model.py           # Neural language model architectures
â”‚   â”œâ”€â”€ train.py           # Main training script
â”‚   â”œâ”€â”€ evaluate.py        # Model evaluation
â”‚   â”œâ”€â”€ generate.py        # Text generation
â”‚   â”œâ”€â”€ utils.py           # Helper functions
â”‚   â””â”€â”€ visualize.py       # Plotting and visualization
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ Pride_and_Prejudice-Jane_Austen.txt
â”œâ”€â”€ models/                # Saved model checkpoints
â”‚   â”œâ”€â”€ underfit/
â”‚   â”œâ”€â”€ overfit/
â”‚   â””â”€â”€ best_fit/
â”œâ”€â”€ plots/                 # Training curves and visualizations
â”œâ”€â”€ logs/                  # Training logs (JSON)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ”„ Reproducibility

All experiments use fixed random seeds for reproducibility:

```python
RANDOM_SEED = 42
```

Seeds are set for:
- Python's random module
- NumPy
- PyTorch (CPU and CUDA)

## ğŸ“¥ Trained Models

Each scenario includes:
- `best_model.pt` - Model with best validation loss
- `final_model.pt` - Model after all training epochs
- Training logs (JSON)
- Loss curves (PNG)

### Loading a Trained Model

```python
import torch
from src.model import create_model

# Load checkpoint
checkpoint = torch.load('models/best_fit/best_model.pt')
config = checkpoint['config']

# Create and load model
model = create_model(
    model_type='LSTM',
    vocab_size=checkpoint['vocab_size'],
    embedding_dim=config['embedding_dim'],
    hidden_dim=config['hidden_dim'],
    num_layers=config['num_layers'],
    dropout=config['dropout']
)
model.load_state_dict(checkpoint['model_state_dict'])
```

## ğŸ› ï¸ Technical Details

### Model Architecture
- **Embedding Layer:** Converts token indices to dense vectors
- **LSTM Layers:** Process sequences and capture long-term dependencies
- **Dropout:** Regularization to prevent overfitting
- **Output Layer:** Projects hidden states to vocabulary size
- **Loss Function:** Cross-entropy loss

### Training Procedure
- **Optimizer:** Adam with weight decay (L2 regularization)
- **Gradient Clipping:** Prevents exploding gradients (threshold: 5.0)
- **Early Stopping:** Stops training if validation loss doesn't improve (patience: 5)
- **Checkpointing:** Saves best model based on validation loss

### Evaluation Metrics
- **Loss:** Cross-entropy loss on test set
- **Perplexity:** exp(loss) - measures model uncertainty
  - Lower is better
  - Typical range: 50-200 for good models

## ğŸ“ Key Learnings

### Underfitting
- **Cause:** Insufficient model capacity or training
- **Symptoms:** High training and validation loss
- **Solution:** Increase model size or training duration

### Overfitting
- **Cause:** Model memorizes training data
- **Symptoms:** Low training loss, high validation loss (gap)
- **Solution:** Add regularization (dropout, weight decay, early stopping)

### Best Fit
- **Goal:** Balance between underfitting and overfitting
- **Indicators:** Small gap between training and validation loss
- **Techniques:** Proper regularization, appropriate model capacity

## ğŸš€ Future Improvements

- [ ] Implement Transformer architecture
- [ ] Add word-level tokenization option
- [ ] Implement beam search for generation
- [ ] Add attention visualization
- [ ] Deploy interactive web demo
- [ ] Experiment with different architectures (GRU, Bidirectional)

